const axios = require('axios');

class AIServerManager {
  constructor() {
    this.serverType = process.env.AI_SERVER_TYPE || 'ollama';
    this.ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434';
    this.lmstudioUrl = process.env.LMSTUDIO_URL || 'http://localhost:1234';
    
    console.log(`ðŸ¤– AI ì„œë²„ ê´€ë¦¬ìž ì´ˆê¸°í™”: ${this.serverType.toUpperCase()}`);
  }

  // AI ì„œë²„ URL ë°˜í™˜
  getServerUrl() {
    return this.serverType === 'lmstudio' ? this.lmstudioUrl : this.ollamaUrl;
  }

  // ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ë°˜í™˜
  getChatEndpoint() {
    return this.serverType === 'lmstudio' ? '/v1/chat/completions' : '/api/chat';
  }

  // ëª¨ë¸ ëª©ë¡ ì—”ë“œí¬ì¸íŠ¸ ë°˜í™˜
  getModelsEndpoint() {
    return this.serverType === 'lmstudio' ? '/v1/models' : '/api/tags';
  }

  // ì„œë²„ íƒ€ìž… ë³€ê²½
  setServerType(type) {
    if (['ollama', 'lmstudio'].includes(type)) {
      this.serverType = type;
      console.log(`ðŸ”„ AI ì„œë²„ íƒ€ìž… ë³€ê²½: ${type.toUpperCase()}`);
      return true;
    }
    return false;
  }

  // ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
  async testConnection() {
    try {
      const url = this.getServerUrl();
      const endpoint = this.getModelsEndpoint();
      const response = await axios.get(`${url}${endpoint}`, { timeout: 10000 });
      
      return {
        success: true,
        serverType: this.serverType,
        url: url,
        status: response.status,
        data: response.data
      };
    } catch (error) {
      return {
        success: false,
        serverType: this.serverType,
        url: this.getServerUrl(),
        error: error.message
      };
    }
  }

  // ì±„íŒ… ìš”ì²­ ì „ì†¡
  async sendChatRequest(messages, model, temperature = 0.7, maxTokens = null) {
    try {
      const url = this.getServerUrl();
      const endpoint = this.getChatEndpoint();
      
      let requestData;
      
      if (this.serverType === 'lmstudio') {
        // LM Studio (OpenAI í˜¸í™˜) í˜•ì‹
        requestData = {
          model: model,
          messages: messages,
          temperature: temperature,
          max_tokens: maxTokens || 1000,
          stream: false
        };
      } else {
        // Ollama í˜•ì‹
        requestData = {
          model: model,
          messages: messages,
          stream: false,
          options: {
            temperature: temperature,
            ...(maxTokens && { num_predict: maxTokens })
          }
        };
      }

      const response = await axios.post(`${url}${endpoint}`, requestData, {
        timeout: 120000,
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'application/json'
        }
      });

      return {
        success: true,
        data: response.data,
        serverType: this.serverType
      };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        serverType: this.serverType
      };
    }
  }

  // ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
  async getModels() {
    try {
      const url = this.getServerUrl();
      const endpoint = this.getModelsEndpoint();
      const response = await axios.get(`${url}${endpoint}`, { timeout: 10000 });
      
      let models = [];
      
      if (this.serverType === 'lmstudio') {
        // LM Studio í˜•ì‹: { data: [{ id: "model_name" }] }
        models = response.data.data?.map(model => model.id) || [];
      } else {
        // Ollama í˜•ì‹: { models: [{ name: "model_name" }] }
        models = response.data.models?.map(model => model.name) || [];
      }
      
      return {
        success: true,
        models: models,
        serverType: this.serverType
      };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        serverType: this.serverType
      };
    }
  }

  // í˜„ìž¬ ì„¤ì • ì •ë³´ ë°˜í™˜
  getConfig() {
    return {
      serverType: this.serverType,
      ollamaUrl: this.ollamaUrl,
      lmstudioUrl: this.lmstudioUrl,
      currentUrl: this.getServerUrl(),
      chatEndpoint: this.getChatEndpoint(),
      modelsEndpoint: this.getModelsEndpoint()
    };
  }
}

module.exports = AIServerManager;

